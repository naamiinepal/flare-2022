seed_everything: 42
trainer:
  logger:
      class_path: pytorch_lightning.loggers.TensorBoardLogger
      init_args:
          save_dir: tb_logs
          name: c2f-fine-unet-l5-s16
          log_graph: True
          default_hp_metric: False
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 10
        monitor: ${model.monitor}
        mode: min
        dirpath: checkpoints/c2f-fine-unet-l5-s16
        save_weights_only: True
        filename: "{epoch:02d}-{val\/loss:.2f}"
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: ${model.monitor}
        patience: 40
        verbose: True
  devices: [0] # None can not be returned in multiple GPUs
  max_epochs: 200
  accelerator: "gpu"
  log_every_n_steps: 10
  # accumulate_grad_batches: 128 # Actual batch size
  val_check_interval: 0.5
  # fast_dev_run: True
model:
  coarse_model:
    class_path: monai.networks.nets.UNet
    init_args:
      spatial_dims: 3
      in_channels: 1
      out_channels: ${data.num_labels_with_bg}
      channels: [8, 16, 32, 64, 128]
      strides: [2, 2, 2, 2]
      # num_res_units: 3
      act: relu
  fine_model:
    class_path: monai.networks.nets.UNet
    init_args:
      spatial_dims: 3
      in_channels: 1
      out_channels: ${data.num_labels_with_bg}
      channels: [16, 32, 64, 128, 256]
      strides: [2, 2, 2, 2]
      # num_res_units: 3
      act: relu
  is_coarse: False
  learning_rate: 0.05
  pseudo_threshold: 0.95
  unsup_weight: 0.005
  sw_batch_size: 4
  sw_overlap: 0.25
  plateu_patience: 3
  plateu_factor: 0.2
  monitor: val/loss
  coarse_model_weights_path: None
  fine_model_weights_path: None
data:
  supervised_dir: /mnt/HDD2/flare2022/datasets/FLARE2022/Training/FLARE22_LabeledCase50
  semisupervised_dir: /mnt/HDD2/flare2022/datasets/FLARE2022/Training/Unlabeled
  do_semi: False
  semi_mu: null
  num_labels_with_bg: 14
  val_ratio: 0.2
  crop_num_samples: 2
  batch_size: ${data.crop_num_samples}
  ds_cache_type: disk
  max_workers: 4
  coarse_roi_size: [128, 128, 64]
  fine_roi_size: [224, 224, 96]
  is_coarse: False